{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoB2CN0XUgIrWA8wW+FrSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil3Vedi/Semi-Supervised-Mail-Classifier/blob/master/Spam_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRAVKxHM15D5",
        "outputId": "47911d0e-11c6-431a-9979-49b81adba3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os #for file handling\n",
        "import numpy as np #for mathematical functions\n",
        "import re #for regular expressions\n",
        "import nltk  # for text processing\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "\n",
        "main_dict = {} #main  dictionary that stores a count of all words across the training data.\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwvzEgJe2ADr"
      },
      "source": [
        "#Generating indices for training and testing data\n",
        "\n",
        "def segregateIndices(SPAMDIR,HAMDIR,train_percentage):\n",
        "  length_spam = len(os.listdir(SPAMDIR))\n",
        "  length_ham = len(os.listdir(HAMDIR))\n",
        "  length_train_spam = int(length_spam*train_percentage/100)\n",
        "  length_test_spam = length_spam - length_train_spam\n",
        "  length_train_ham = int(length_ham*train_percentage/100)\n",
        "  length_test_ham = length_ham - length_train_ham\n",
        "\n",
        "  spam_indices = [x for x in range(length_spam)]\n",
        "  ham_indices = [y for y in range(length_ham)]\n",
        "  np_spam_indices = np.array(spam_indices)\n",
        "  np_ham_indices = np.array(ham_indices)\n",
        "\n",
        "  np.random.shuffle(np_spam_indices)\n",
        "  np.random.shuffle(np_ham_indices)\n",
        "\n",
        "  train_spam_indices = np_spam_indices[:length_train_spam]\n",
        "  train_ham_indices = np_ham_indices[:length_train_ham]\n",
        "  test_spam_indices = np_spam_indices[-length_test_spam:]\n",
        "  test_ham_indices = np_ham_indices[-length_test_ham:]\n",
        "\n",
        "  print(\"Total Spam: \" + str(length_spam))\n",
        "  print(\"Total Ham: \" + str(length_ham))\n",
        "  print(\"Total Train Spam: \" + str(len(train_spam_indices)))\n",
        "  print(\"Total Train Ham: \" + str(len(train_ham_indices)))\n",
        "  print(\"Total Test Spam: \" + str(len(test_spam_indices)))\n",
        "  print(\"Total Test Ham: \" + str(len(test_ham_indices)))\n",
        "\n",
        "  return_list = [train_spam_indices,train_ham_indices,test_spam_indices,test_ham_indices]\n",
        "  return return_list\n",
        "\n",
        "#scanning a mail to update the main dictionary\n",
        "def scanMail(mail_string):\n",
        "  res = re.findall(r'\\w+', mail_string)\n",
        "  res.pop(0) #Removing first word from the mail (Usually \"subject\")\n",
        "  final_res = []\n",
        "  for each_word in res:\n",
        "    if ((each_word.isalpha()) and (each_word not in stop_words)) :\n",
        "      final_res.append(each_word) #updating main dictionary\n",
        "      if each_word in main_dict:\n",
        "        main_dict[each_word] += 1\n",
        "      else:\n",
        "        main_dict[each_word] = 1\n",
        "\n",
        "  return final_res #for debugging"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lGxvhF12C7B",
        "outputId": "edfe608a-5496-45dc-f904-29d43e6c1036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#Segregating Training and Testing Data from Drive\n",
        "\n",
        "ENRONSPAMDATASET = 'drive/My Drive/Enron Spam/spam'\n",
        "ENRONNOSPAMDATASET = 'drive/My Drive/Enron Spam/ham'\n",
        "\n",
        "indices = segregateIndices(ENRONSPAMDATASET,ENRONNOSPAMDATASET,70)\n",
        "\n",
        "training_spam_indices = indices[0]\n",
        "training_ham_indices = indices[1]\n",
        "testing_spam_indices = indices[2]\n",
        "testing_ham_indices = indices[3]\n",
        "spam_files = os.listdir(ENRONSPAMDATASET)\n",
        "ham_files = os.listdir(ENRONNOSPAMDATASET)\n",
        "\n",
        "for each_index in training_spam_indices:\n",
        "  each_file = spam_files[each_index]\n",
        "  FILENAME = os.path.join(ENRONSPAMDATASET, each_file)\n",
        "  with open(FILENAME, 'r',encoding='utf-8',errors='ignore') as myfile:\n",
        "    data = myfile.read()\n",
        "    word_list = scanMail(data)\n",
        "\n",
        "for each_index in training_ham_indices:\n",
        "  each_file = ham_files[each_index]\n",
        "  FILENAME = os.path.join(ENRONNOSPAMDATASET, each_file)\n",
        "  with open(FILENAME, 'r',encoding='utf-8',errors='ignore') as myfile:\n",
        "    data = myfile.read()\n",
        "    word_list = scanMail(data)\n",
        "\n",
        "# Forming a new dictionary with the N most frequent words from the main dictionary\n",
        "\n",
        "def getCommon(new_dict, n):\n",
        "  temp_dict = new_dict\n",
        "  ret_dict = {}\n",
        "  for i in range(n):\n",
        "    Keymax = max(temp_dict, key=temp_dict.get)\n",
        "    ret_dict[Keymax]=temp_dict[Keymax]\n",
        "    del temp_dict[Keymax]\n",
        "  return ret_dict"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Spam: 1518\n",
            "Total Ham: 3681\n",
            "Total Train Spam: 1062\n",
            "Total Train Ham: 2576\n",
            "Total Test Spam: 456\n",
            "Total Test Ham: 1105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UezTleOi2FOH"
      },
      "source": [
        "final_dict = getCommon(main_dict,1000)\n",
        "\n",
        "#saving Bag of Words as a CSV File\n",
        "import csv\n",
        "with open('drive/My Drive/Enron Spam/bagofwords.csv', 'w') as f:\n",
        "    for key in final_dict.keys():\n",
        "        f.write(\"%s,%s\\n\"%(key,final_dict[key]))\n",
        "\n",
        "#exporting training and testing indices as an NPY file\n",
        "np.save('drive/My Drive/Enron Spam/spam_classifier_indices.npy', indices)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTqiBQWl2IsW"
      },
      "source": [
        "from sklearn.utils import shuffle #for shuffling datasets\n",
        "import pandas as pd #for manipulating dataframes\n",
        "\n",
        "#generating feature vector from a mail\n",
        "def generateVector(mail_string,bagofwords):\n",
        "  mail_vector = np.zeros(1000)\n",
        "  res = re.findall(r'\\w+', mail_string)\n",
        "  res.pop(0)\n",
        "  final_res = []\n",
        "  for each_word in res:\n",
        "    if ((each_word.isalpha()) and (each_word not in stop_words)) :\n",
        "      final_res.append(each_word)\n",
        "  vector_counter = 0\n",
        "  for each_word in bagofwords:\n",
        "    if each_word in final_res:\n",
        "      mail_vector[vector_counter]=1\n",
        "    vector_counter+=1\n",
        "\n",
        "  return mail_vector\n",
        "\n",
        "BAGOFWORDS = 'drive/My Drive/Enron Spam/bagofwords.csv'\n",
        "bagofwords_data = pd.read_csv(BAGOFWORDS)\n",
        "bagofwords = bagofwords_data[bagofwords_data.columns[0]]\n",
        "\n",
        "SPAM_CLASSIFIER_INDICES = 'drive/My Drive/Enron Spam/spam_classifier_indices.npy'\n",
        "spam_classifier_indices = np.load(SPAM_CLASSIFIER_INDICES,allow_pickle=True)\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "X_test = []\n",
        "Y_test = []"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvP3EeyO2NyZ",
        "outputId": "e0580d67-a5c0-4a51-fe40-c8c299e442f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "spam_training_indices = spam_classifier_indices[0]\n",
        "ham_training_indices = spam_classifier_indices[1]\n",
        "spam_testing_indices = spam_classifier_indices[2]\n",
        "ham_testing_indices = spam_classifier_indices[3]\n",
        "\n",
        "#Adding Spam Mails to X_test and Y_test\n",
        "print(\"Adding Spam Mails to X_test and Y_test, Please be Patient...\")\n",
        "file_list = os.listdir(ENRONSPAMDATASET)\n",
        "for each_index in spam_testing_indices:\n",
        "  each_file = file_list[each_index]\n",
        "  FILENAME = os.path.join(ENRONSPAMDATASET, each_file)\n",
        "  with open(FILENAME, 'r',encoding='utf-8',errors='ignore') as myfile:\n",
        "    data = myfile.read()\n",
        "    mail_vector = generateVector(data,bagofwords)\n",
        "    X_test.append(mail_vector)\n",
        "    Y_test.append(1)\n",
        "\n",
        "\n",
        "#Adding Non Spam Mails to X_test and Y_test\n",
        "print(\"Adding Non Spam Mails to X_test and Y_test, Please be Patient...\")\n",
        "file_list = os.listdir(ENRONNOSPAMDATASET)\n",
        "for each_index in ham_testing_indices:\n",
        "  each_file = file_list[each_index]\n",
        "  FILENAME = os.path.join(ENRONNOSPAMDATASET, each_file)\n",
        "  with open(FILENAME, 'r',encoding='utf-8',errors='ignore') as myfile:\n",
        "    data = myfile.read()\n",
        "    mail_vector = generateVector(data,bagofwords)\n",
        "    X_test.append(mail_vector)\n",
        "    Y_test.append(0)\n",
        "\n",
        "#Adding Spam Mails to X_train and Y_train\n",
        "print(\"Adding Spam Mails to X_train and Y_train, Please be Patient...\")\n",
        "file_list = os.listdir(ENRONSPAMDATASET)\n",
        "for each_index in spam_training_indices:\n",
        "  each_file = file_list[each_index]\n",
        "  FILENAME = os.path.join(ENRONSPAMDATASET, each_file)\n",
        "  with open(FILENAME, 'r',encoding='utf-8',errors='ignore') as myfile:\n",
        "    data = myfile.read()\n",
        "    mail_vector = generateVector(data,bagofwords)\n",
        "    X_train.append(mail_vector)\n",
        "    Y_train.append(1)\n",
        "\n",
        "#Adding Non Spam Mails to X_train and Y_train\n",
        "print(\"Adding Non Spam Mails to X_train and Y_train, Please be Patient...\")\n",
        "file_list = os.listdir(ENRONNOSPAMDATASET)\n",
        "for each_index in ham_training_indices:\n",
        "  each_file = file_list[each_index]\n",
        "  FILENAME = os.path.join(ENRONNOSPAMDATASET, each_file)\n",
        "  with open(FILENAME, 'r',encoding='utf-8',errors='ignore') as myfile:\n",
        "    data = myfile.read()\n",
        "    mail_vector = generateVector(data,bagofwords)\n",
        "    X_train.append(mail_vector)\n",
        "    Y_train.append(0)\n",
        "\n",
        "#saving numpy arrays\n",
        "XTRAIN = 'drive/My Drive/Enron Spam/spam_classifier_xtrain.npy'\n",
        "YTRAIN = 'drive/My Drive/Enron Spam/spam_classifier_ytrain.npy'\n",
        "XTEST = 'drive/My Drive/Enron Spam/spam_classifier_xtest.npy'\n",
        "YTEST = 'drive/My Drive/Enron Spam/spam_classifier_ytest.npy'\n",
        "\n",
        "np.save(XTRAIN,X_train)\n",
        "np.save(YTRAIN,Y_train)\n",
        "np.save(XTEST, X_test)\n",
        "np.save(YTEST, Y_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding Spam Mails to X_test and Y_test, Please be Patient...\n",
            "Adding Non Spam Mails to X_test and Y_test, Please be Patient...\n",
            "Adding Spam Mails to X_train and Y_train, Please be Patient...\n",
            "Adding Non Spam Mails to X_train and Y_train, Please be Patient...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq1lmelm2PZr",
        "outputId": "43a23a5e-42bb-476e-843b-397d4bee64c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix #to describe performance\n",
        "from sklearn.naive_bayes import MultinomialNB #enables Multinomial Naive Bayes Classifier\n",
        "from sklearn.svm import LinearSVC #enables Support Vector Classifier\n",
        "from sklearn.metrics import accuracy_score #to describe performance\n",
        "\n",
        "XTRAIN_LOC = 'drive/My Drive/Enron Spam/spam_classifier_xtrain.npy'\n",
        "YTRAIN_LOC = 'drive/My Drive/Enron Spam/spam_classifier_ytrain.npy'\n",
        "XTEST_LOC = 'drive/My Drive/Enron Spam/spam_classifier_xtest.npy'\n",
        "YTEST_LOC = 'drive/My Drive/Enron Spam/spam_classifier_ytest.npy'\n",
        "\n",
        "X_train = np.load(XTRAIN_LOC)\n",
        "Y_train = np.load(YTRAIN_LOC)\n",
        "X_test = np.load(XTEST_LOC)\n",
        "Y_test = np.load(YTEST_LOC)\n",
        "\n",
        "#Training SVM and Naive Bayes Classifier\n",
        "\n",
        "model1 = MultinomialNB()\n",
        "model2 = LinearSVC()\n",
        "model1.fit(X_train, Y_train)\n",
        "model2.fit(X_train, Y_train)\n",
        "\n",
        "result1 = model1.predict(X_test)\n",
        "result2 = model2.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(Y_test,result1))\n",
        "print(accuracy_score(Y_test,result1))\n",
        "print(confusion_matrix(Y_test,result2))\n",
        "print(accuracy_score(Y_test,result2))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1063   42]\n",
            " [ 152  304]]\n",
            "0.8757206918641897\n",
            "[[1065   40]\n",
            " [ 151  305]]\n",
            "0.877642536835362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t-vtGqJ2Ra_",
        "outputId": "9700102c-5728-4262-85dd-18c760366f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Libraries to train the Neural Network\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "import random\n",
        "\n",
        "#converting binary classifications to categorical ones\n",
        "Y_train_categorical = to_categorical(Y_train)\n",
        "Y_test_categorical = to_categorical(Y_test)\n",
        "\n",
        "#Initialising Neural Network\n",
        "neuralNet = tf.keras.Sequential([\n",
        "# Adds a densely-connected layer with 64 units to the model:\n",
        "layers.Dense(64, activation='relu', input_shape=(1000,)),\n",
        "# Add another:\n",
        "layers.Dense(64, activation='relu'),\n",
        "# Add an output layer with 2 output units:\n",
        "layers.Dense(2, activation='softmax')])\n",
        "\n",
        "neuralNet.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "neuralNet.fit(X_train, Y_train_categorical, epochs=10, batch_size=32)\n",
        "Y_pred = neuralNet.predict(X_test)\n",
        "\n",
        "#converting Y_pred to multiclass predictions to single valued outputs\n",
        "Y_pred_normalised = []\n",
        "for each_output in Y_pred:\n",
        "  if (each_output[0]>each_output[1]):\n",
        "    Y_pred_normalised.append(0)\n",
        "  else:\n",
        "    Y_pred_normalised.append(1)\n",
        "Y_pred_np = np.array(Y_pred_normalised)\n",
        "\n",
        "#performance of the neural network\n",
        "print(confusion_matrix(Y_test,Y_pred_np))\n",
        "print(accuracy_score(Y_test,Y_pred_np))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.8290\n",
            "Epoch 2/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4175 - accuracy: 0.8947\n",
            "Epoch 3/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8975\n",
            "Epoch 4/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4073 - accuracy: 0.9057\n",
            "Epoch 5/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.9076\n",
            "Epoch 6/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.9101\n",
            "Epoch 7/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.4019 - accuracy: 0.9107\n",
            "Epoch 8/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.9148\n",
            "Epoch 9/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.3974 - accuracy: 0.9156\n",
            "Epoch 10/10\n",
            "114/114 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.9159\n",
            "[[1064   41]\n",
            " [ 164  292]]\n",
            "0.8686739269698911\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}